{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7653cb1b",
   "metadata": {},
   "source": [
    "## Keys for Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c81412d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 13:20:34.410 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-11 13:20:34.992 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\soarora\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-12-11 13:20:34.992 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-11 13:20:34.995 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-11 13:20:34.995 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-11 13:20:34.995 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-11 13:20:34.999 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "st.title(\"My Streamlit App\")\n",
    "st.write(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c26827d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "AZURE_SEARCH_SERVICE: str = os.getenv(\"AZURE_SEARCH_SERVICE\")\n",
    "AZURE_SEARCH_KEY: str = os.getenv(\"AZURE_SEARCH_KEY\")\n",
    "AZURE_OPENAI_ACCOUNT: str = os.getenv(\"AZURE_OPENAI_ACCOUNT\")\n",
    "AZURE_OPENAI_KEY: str = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "AZURE_AI_MULTISERVICE_ACCOUNT: str = os.getenv(\"AZURE_AI_MULTISERVICE_ACCOUNT\")\n",
    "AZURE_AI_MULTISERVICE_KEY: str = os.getenv(\"AZURE_AI_MULTISERVICE_KEY\")\n",
    "AZURE_STORAGE_CONNECTION: str = os.getenv(\"AZURE_STORAGE_CONNECTION\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7fe838e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://capstoneindex.search.windows.net\n"
     ]
    }
   ],
   "source": [
    "print(AZURE_SEARCH_SERVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6a6a50",
   "metadata": {},
   "source": [
    "## Creating an Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0f787160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created: sow-index\n"
     ]
    }
   ],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch,\n",
    "    SearchIndex\n",
    ")\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "index_name = \"sow-index\"\n",
    "index_client = SearchIndexClient(endpoint= AZURE_SEARCH_SERVICE, credential = credential)\n",
    "\n",
    "field = [\n",
    "    SearchField(name=\"id\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"title\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"chunk_id\", type=SearchFieldDataType.String, sortable=True, filterable=True, key=True, facetable=True, analyzer_name=\"keyword\"),\n",
    "    SearchField(name=\"chunk\", type=SearchFieldDataType.String, sortable=False, filterable=False, facetable=False),\n",
    "    SearchField(\n",
    "        name=\"text_vector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        vector_search_dimensions=3072,\n",
    "        vector_search_profile_name=\"myHnswProfile\"\n",
    "    )\n",
    "]\n",
    "\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[HnswAlgorithmConfiguration(name=\"myHnsw\"),\n",
    "                ],\n",
    "                profiles=[\n",
    "                    VectorSearchProfile(\n",
    "                        name=\"myHnswProfile\",\n",
    "                        algorithm_configuration_name=\"myHnsw\",\n",
    "                        vectorizer_name=\"myOpenAIVectorizer\"\n",
    "                    )\n",
    "                ],\n",
    "                vectorizers=[\n",
    "                    AzureOpenAIVectorizer(\n",
    "                        vectorizer_name=\"myOpenAIVectorizer\",\n",
    "                        kind = \"azureOpenAI\",\n",
    "                        parameters=AzureOpenAIVectorizerParameters(\n",
    "                            resource_url=AZURE_OPENAI_ACCOUNT,\n",
    "                            deployment_name=\"text-embedding-3-large\",\n",
    "                            model_name=\"text-embedding-3-large\",\n",
    "                            api_key=AZURE_OPENAI_KEY\n",
    "                        )\n",
    "                    )\n",
    "                ],\n",
    "            )\n",
    "\n",
    "# Semantic understanding rather than just keyword matching or vector similarity alone\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field = SemanticField(field_name=\"title\"),\n",
    "        content_fields = [SemanticField(field_name=\"chunk\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "# Create the index\n",
    "index = SearchIndex(\n",
    "    name=index_name,\n",
    "    fields=field,\n",
    "    vector_search=vector_search,\n",
    "    semantic_search=semantic_search\n",
    ")\n",
    "\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(\"Index created:\", result.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d334418c",
   "metadata": {},
   "source": [
    "## Connecting Index to Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2e9533ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source created: sow-datasource\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes import SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndexerDataContainer,\n",
    "    SearchIndexerDataSourceConnection)\n",
    "\n",
    "indexer_client = SearchIndexerClient(endpoint= AZURE_SEARCH_SERVICE, credential = credential)\n",
    "\n",
    "container = SearchIndexerDataContainer(name=\"sow-container\")\n",
    "data_source_connection = SearchIndexerDataSourceConnection(\n",
    "    name=\"sow-datasource\",\n",
    "    type=\"azureblob\",\n",
    "    connection_string=AZURE_STORAGE_CONNECTION,\n",
    "    container=container\n",
    ")\n",
    "\n",
    "data_source = indexer_client.create_or_update_data_source_connection(data_source_connection)\n",
    "print(\"Data source created:\", data_source.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c484c4f2",
   "metadata": {},
   "source": [
    "## Skillset for Chunking and Embeddings Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "10ead47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skillset created: statement-of-work-skillset\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SplitSkill,\n",
    "    InputFieldMappingEntry,\n",
    "    OutputFieldMappingEntry,\n",
    "    AzureOpenAIEmbeddingSkill,\n",
    "    SearchIndexerIndexProjection,\n",
    "    SearchIndexerIndexProjectionSelector,\n",
    "    SearchIndexerIndexProjectionsParameters,\n",
    "    IndexProjectionMode,\n",
    "    CognitiveServicesAccountKey,\n",
    "    SearchIndexerSkillset\n",
    ")\n",
    "\n",
    "skillset_name = \"statement-of-work-skillset\"\n",
    "\n",
    "split_skill = SplitSkill(\n",
    "    name=\"split-skill\",\n",
    "    description=\"Splits skill to chunk documents\",\n",
    "    text_split_mode=\"pages\",\n",
    "    context=\"/document\",\n",
    "    inputs = [InputFieldMappingEntry(name=\"text\", source=\"/document/content\")],\n",
    "    outputs = [OutputFieldMappingEntry(name=\"textItems\", target_name=\"pages\")], # Produces chunked text items called \"textItems\" and stores them in a field named \"pages\"\n",
    "    maximum_page_length=2000, # maximum characters in each chunk/page\n",
    "    page_overlap_length=500\n",
    ")\n",
    "\n",
    "embedding_skill = AzureOpenAIEmbeddingSkill(\n",
    "    description=\"Generates embeddings for text chunks\",\n",
    "    name=\"azure-openai-embedding-skill\",\n",
    "    context=\"/document/pages/*\", # apply to each chunk in the page\n",
    "    resource_url=AZURE_OPENAI_ACCOUNT,\n",
    "    deployment_name=\"text-embedding-3-large\",\n",
    "    model_name=\"text-embedding-3-large\",\n",
    "    api_key=AZURE_OPENAI_KEY,\n",
    "    inputs=[InputFieldMappingEntry(name=\"text\", source=\"/document/pages/*\")],\n",
    "    outputs=[OutputFieldMappingEntry(name=\"embedding\", target_name=\"text_vector\")], \n",
    ")\n",
    "\n",
    "# map enriched data from skillset pipeline to the index\n",
    "\n",
    "index_projections = SearchIndexerIndexProjection (\n",
    "    selectors=[\n",
    "        SearchIndexerIndexProjectionSelector(\n",
    "            target_index_name=index_name, # which index to write to\n",
    "            parent_key_field_name=\"id\", # links each chunk to its parent/source document\n",
    "            source_context=\"/document/pages/*\",\n",
    "            mappings=[ # how to map data to index fields\n",
    "                InputFieldMappingEntry(name=\"chunk\", source=\"/document/pages/*\"),\n",
    "                InputFieldMappingEntry(name=\"text_vector\", source=\"/document/pages/*/text_vector\"),\n",
    "                InputFieldMappingEntry(name=\"title\", source=\"/document/metadata_storage_name\"),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    parameters=SearchIndexerIndexProjectionsParameters(\n",
    "        projection_mode=IndexProjectionMode.SKIP_INDEXING_PARENT_DOCUMENTS #only index the chunk, not the entire parent document\n",
    "    )\n",
    ")\n",
    "\n",
    "cognitive_services_account = CognitiveServicesAccountKey(\n",
    "    key=AZURE_AI_MULTISERVICE_KEY)\n",
    "skills = [split_skill, embedding_skill]\n",
    "\n",
    "skillset = SearchIndexerSkillset(\n",
    "    name=skillset_name,\n",
    "    description=\"Skillset for processing statement of work documents\",\n",
    "    skills=skills,\n",
    "    index_projection=index_projections,\n",
    "    cognitive_services_account=cognitive_services_account)\n",
    "\n",
    "client = SearchIndexerClient(endpoint= AZURE_SEARCH_SERVICE, credential = credential)\n",
    "created_skillset = client.create_or_update_skillset(skillset)\n",
    "print(\"Skillset created:\", created_skillset.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc284d7",
   "metadata": {},
   "source": [
    "## Create an Indexer\n",
    "\n",
    "The indexer is the orchestrator that automates the entire data ingestion pipeline. \n",
    "\n",
    "Pulls data -> Chunking + Embeddings -> Write processed data to target index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5adccd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexer created: sow-indexer\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import SearchIndexer\n",
    "\n",
    "indexer_name = \"sow-indexer\"\n",
    "indexer_parameters = None\n",
    "\n",
    "indexer = SearchIndexer(\n",
    "    name=indexer_name,\n",
    "    description=\"Indexer for statement of work documents\",\n",
    "    skillset_name=skillset_name,\n",
    "    target_index_name=index_name,\n",
    "    data_source_name=data_source.name,\n",
    "    parameters=indexer_parameters\n",
    ")\n",
    "\n",
    "indexer_client = SearchIndexerClient(endpoint= AZURE_SEARCH_SERVICE, credential = credential)\n",
    "created_indexer = indexer_client.create_or_update_indexer(indexer)\n",
    "print(\"Indexer created:\", created_indexer.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded67327",
   "metadata": {},
   "source": [
    "## Check indexer status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8837de4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexer status: running\n"
     ]
    }
   ],
   "source": [
    "status = indexer_client.get_indexer_status(indexer_name)\n",
    "print(\"Indexer status:\", status.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "262d224e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexer Status: running\n",
      "Last Result: success\n",
      "\n",
      "Execution Summary:\n",
      "  - Items Processed: 1\n",
      "  - Items Failed: 0\n",
      "  - Start Time: 2025-12-11 07:02:46.312000+00:00\n",
      "  - End Time: 2025-12-11 07:02:54.144000+00:00\n",
      "\n",
      "✓ Indexer is currently running. Wait for it to complete.\n"
     ]
    }
   ],
   "source": [
    "# Check detailed indexer execution history\n",
    "status = indexer_client.get_indexer_status(indexer_name)\n",
    "\n",
    "print(f\"Indexer Status: {status.status}\")\n",
    "print(f\"Last Result: {status.last_result.status if status.last_result else 'No runs yet'}\")\n",
    "\n",
    "if status.last_result:\n",
    "    print(f\"\\nExecution Summary:\")\n",
    "    print(f\"  - Items Processed: {status.last_result.item_count}\")\n",
    "    print(f\"  - Items Failed: {status.last_result.failed_item_count}\")\n",
    "    print(f\"  - Start Time: {status.last_result.start_time}\")\n",
    "    print(f\"  - End Time: {status.last_result.end_time}\")\n",
    "    \n",
    "    if status.last_result.errors:\n",
    "        print(f\"\\n❌ ERRORS ({len(status.last_result.errors)}):\")\n",
    "        for error in status.last_result.errors[:5]:  # Show first 5 errors\n",
    "            print(f\"  - Key: {error.key}\")\n",
    "            print(f\"    Error: {error.error_message}\")\n",
    "            print(f\"    Details: {error.details if hasattr(error, 'details') else 'N/A'}\")\n",
    "            print()\n",
    "    \n",
    "    if status.last_result.warnings:\n",
    "        print(f\"\\n⚠️  WARNINGS ({len(status.last_result.warnings)}):\")\n",
    "        for warning in status.last_result.warnings[:5]:\n",
    "            print(f\"  - Key: {warning.key}\")\n",
    "            print(f\"    Warning: {warning.message}\")\n",
    "            print()\n",
    "\n",
    "# Check if indexer needs to run\n",
    "if status.status == \"running\":\n",
    "    print(\"\\n✓ Indexer is currently running. Wait for it to complete.\")\n",
    "elif status.last_result is None:\n",
    "    print(\"\\n⚠️  Indexer has never run. Running it now...\")\n",
    "    indexer_client.run_indexer(indexer_name)\n",
    "    print(\"Indexer started. Check status again in a few moments.\")\n",
    "else:\n",
    "    print(f\"\\n✓ Indexer last ran: {status.last_result.end_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c1c909",
   "metadata": {},
   "source": [
    "## Adding the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5254a82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The Contoso project includes the following key deliverables:\n",
      "\n",
      "- **Disaster Recovery and Business Continuity Planning**: Includes the planning and execution of measures to ensure disaster recovery and business continuity.\n",
      "- **Tiered Support Plan**: Development of a plan for tiered support of the application.\n",
      "- **Adoption Change Management**: A plan for training users on the newly deployed TPA application to maintain productivity.\n",
      "- **User Acceptance Testing (UAT)**: Includes a plan for UAT execution, as well as preparation of UAT test data and test cases.\n",
      "- **Security Procedures and Policies**: Sharing of security procedures and policies before the project starts.\n",
      "\n",
      "Additionally, service deliverables are categorized into:\n",
      "- **Document Deliverables**: Such as Word, Excel, Visio, or Project files.\n",
      "- **Functioning Components or Solution Deliverables**: Such as epics and features.\n",
      "\n",
      "For deliverables to be accepted:\n",
      "- Contoso will review and approve the service deliverables at specified milestones.\n",
      "- Document deliverables must be reviewed within 10 business days of submittal. They can be accepted by signing and returning the Service Deliverable Acceptance form, or rejected with a complete written list of the reasons.\n",
      "\n",
      "(Source: CONTOSO TPA Core SOW.pdf)\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "from azure.search.documents import SearchClient # client to interact with the Search Index\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "\n",
    "deployment_name = \"gpt-4o\"\n",
    "\n",
    "open_ai_client = AzureOpenAI(\n",
    "    api_version=\"2025-01-01-preview\",\n",
    "    azure_endpoint=AZURE_OPENAI_ACCOUNT,\n",
    "    api_key=AZURE_OPENAI_KEY\n",
    ")\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=AZURE_SEARCH_SERVICE,\n",
    "    index_name=index_name,\n",
    "    credential=AzureKeyCredential(AZURE_SEARCH_KEY)\n",
    ")\n",
    "\n",
    "# Instructions for the model\n",
    "GROUNDED_PROMPT=\"\"\"\n",
    "You are an AI assistant that helps users learn from the information found in the source material.\n",
    "Answer the query using only the sources provided below.\n",
    "Use bullets if the answer has multiple points.\n",
    "If the answer is longer than 3 sentences, provide a summary.\n",
    "Answer ONLY with the facts listed in the list of sources below. Cite your source when you answer the question\n",
    "If there isn't enough information below, say you don't know.\n",
    "Do not generate answers that don't use the sources below.\n",
    "Query: {query}\n",
    "Sources:\\n{sources}\n",
    "\"\"\"\n",
    "\n",
    "query = \"Tell me something about the Contoso project deliverables\"\n",
    "vectorized_query = VectorizableTextQuery(text=query, k_nearest_neighbors=5, fields=[\"text_vector\"])\n",
    "\n",
    "results = search_client.search(\n",
    "    query_type=\"semantic\",\n",
    "    semantic_configuration_name=\"my-semantic-config\",\n",
    "    search_text=query, \n",
    "    vector_queries=[vectorized_query], \n",
    "    select=[\"title\", \"chunk\"],  #which fields to return\n",
    "    top=5)\n",
    "\n",
    "response = open_ai_client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant that helps people find information.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": GROUNDED_PROMPT.format(\n",
    "                query=query,\n",
    "                sources=\"\\n\".join([f\"- {doc['chunk']} (Source: {doc['title']})\" for doc in results])\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "llm_response = response.choices[0].message.content\n",
    "\n",
    "print(\"Answer:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfe957e",
   "metadata": {},
   "source": [
    "## View Documents in Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dacc9210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: sow-index\n",
      "Total fields: 5\n",
      "Approximate document count: 0\n"
     ]
    }
   ],
   "source": [
    "# Get total document count\n",
    "\n",
    "index_stats = index_client.get_index(index_name)\n",
    "print(f\"Index: {index_name}\")\n",
    "print(f\"Total fields: {len(index_stats.fields)}\")\n",
    "\n",
    "# Count documents (approximate)\n",
    "count_results = search_client.search(search_text=\"*\", include_total_count=True)\n",
    "print(f\"Approximate document count: {count_results.get_count()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
